{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f183dd4b",
   "metadata": {},
   "source": [
    "#### Importing Libraries for: stemming the reviews, using Naive Baiyes Classifier, creating a confusion matrix and using Logistic Regression Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b013cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29279b94",
   "metadata": {},
   "source": [
    "# Task 1 - Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235a8b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "ford_reviews = pd.read_csv('car_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0fee32",
   "metadata": {},
   "source": [
    "### Step 1 - Splitting the Data. \n",
    "\n",
    "##### Note: Since the 'int' converter method in python rounds of decimal numbers to the largest integer smaller than it's original value, we need to add one to get the desired 1106:276 split of our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d9503b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a randomized vector whose length is the length of our dataset\n",
    "total_count = ford_reviews.shape[0]\n",
    "np.random.seed(0)\n",
    "shuffle = np.random.permutation(total_count)\n",
    "\n",
    "#Splitting the dataset into 'x' and 'y', so that it can be used in our model\n",
    "#'x' represents the reviews and 'y' represents the sentiments \n",
    "x = ford_reviews.iloc[shuffle, 1]\n",
    "y = ford_reviews.iloc[shuffle, 0]\n",
    "\n",
    "#splitting the dataset in training and testing sets in a 80:20 ratio\n",
    "split = int(total_count * 0.8) + 1  #the required 80% split\n",
    "x_train = x[:split]\n",
    "y_train = y[:split]\n",
    "\n",
    "x_test = x[split:]\n",
    "y_test = y[split:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecb3ed0",
   "metadata": {},
   "source": [
    "#### Splitting the test and train set at beginning ensures that two sets stay seperate till the end. From here onwards, we keep working on the test set and train set seperately. Both Step 2 and Step 3 are performed seperately on the two sets, thus ensuring that the two don't mix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7670a1c",
   "metadata": {},
   "source": [
    "### Step 2 - Data Clean-up\n",
    "\n",
    "##### Creating an all-in-one function that performs all the data cleaning steps in one fell swoop.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74681b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_cleaner(reviews):\n",
    "    #ListOfbow represents the bag of words that we will get\n",
    "    ListOfbow = []\n",
    "    #Getting the list of all stopwords\n",
    "    stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "    porter = PorterStemmer()\n",
    "    for review in range(reviews.shape[0]):\n",
    "        #bowdict represents each review that will be returned as dictionaries\n",
    "        bowdict = {}\n",
    "        #Tokenize our review\n",
    "        original = nltk.word_tokenize(reviews.iloc[review])\n",
    "        for words in original:\n",
    "            #Turn all the words to lowercase\n",
    "            words = words.lower()\n",
    "            #Stems the words\n",
    "            new_word = porter.stem(words)\n",
    "            #If the words is not a stopword and if it is not already accounted for\n",
    "            if words not in stopwords and new_word not in bowdict:\n",
    "                bowdict[new_word] = 1  #account for it\n",
    "            elif words not in stopwords and new_word in bowdict: #else\n",
    "                bowdict[new_word] += 1 #add a count\n",
    "        ListOfbow.append(bowdict)\n",
    "    return ListOfbow\n",
    "\n",
    "#Creating a seperate bag of words for our test and train set\n",
    "BOW_train = review_cleaner(x_train)\n",
    "BOW_test = review_cleaner(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d4ed004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I admit that I m not much of a mechanic so this will not be detailed or technical  I ll just get to the basics  Six years ago  my parents bought a 1996 Ford Windstar  and today  it s a constant pain  We ve had problems with all sorts of noises  there have been times when we had noises whenever the car turned and noises when you apply the brakes  Also  the brake warning light is constantly coming on leading to road trips in which we worry if we ll make it home safely  We usually don t use it for long trips unless we have a lot of stuff to carry  We ve also had transmission problems  The brakes have even stopped working once  However  when we take the van to the Ford place to get it fixed  the mechanics claim that they don t hear the noises  When they do  fix  it  we end up with a different noise the next month  Once  we even had to take it back the week after taking it to the shop  Further taking the car to the shop is a day long event  or longer  and a major incovenience It turns out that the Ford Windstar was recalled because of brake problems but not the 1996 although it appears that there was reason to  Apparently we were among the truly blessed since others have had accidents caused by head gasket problems  The epinions site wouldn t let me put the link here for you  it read it as a word that was too long  but look it up at consumeraffairs com All in all it seems sorry compared to our Toyota Camry which we rarely have to take to the shop and when we do  the service is much better  Over the years we have spent thousands on fixing this car  My mom s credit card bill has risen because of repairs on this minivan  Perhaps you would be blessed to find one of the good ones but I wouldn t risk it  \n",
      " \n",
      "{'admit': 1, 'much': 2, 'mechan': 2, 'detail': 1, 'technic': 1, 'get': 2, 'basic': 1, 'six': 1, 'year': 2, 'ago': 1, 'parent': 1, 'bought': 1, '1996': 2, 'ford': 3, 'windstar': 2, 'today': 1, 'constant': 1, 'pain': 1, 'problem': 4, 'sort': 1, 'nois': 5, 'time': 1, 'whenev': 1, 'car': 3, 'turn': 2, 'appli': 1, 'brake': 4, 'also': 2, 'warn': 1, 'light': 1, 'constantli': 1, 'come': 1, 'lead': 1, 'road': 1, 'trip': 2, 'worri': 1, 'make': 1, 'home': 1, 'safe': 1, 'usual': 1, 'use': 1, 'long': 3, 'unless': 1, 'lot': 1, 'stuff': 1, 'carri': 1, 'transmiss': 1, 'even': 2, 'stop': 1, 'work': 1, 'howev': 1, 'take': 5, 'van': 1, 'place': 1, 'fix': 3, 'claim': 1, 'hear': 1, 'end': 1, 'differ': 1, 'next': 1, 'month': 1, 'back': 1, 'week': 1, 'shop': 3, 'day': 1, 'event': 1, 'longer': 1, 'major': 1, 'incoveni': 1, 'recal': 1, 'although': 1, 'appear': 1, 'reason': 1, 'appar': 1, 'among': 1, 'truli': 1, 'bless': 2, 'sinc': 1, 'other': 1, 'accid': 1, 'caus': 1, 'head': 1, 'gasket': 1, 'epinion': 1, 'site': 1, 'let': 1, 'put': 1, 'link': 1, 'read': 1, 'word': 1, 'look': 1, 'consumeraffair': 1, 'com': 1, 'seem': 1, 'sorri': 1, 'compar': 1, 'toyota': 1, 'camri': 1, 'rare': 1, 'servic': 1, 'better': 1, 'spent': 1, 'thousand': 1, 'mom': 1, 'credit': 1, 'card': 1, 'bill': 1, 'risen': 1, 'repair': 1, 'minivan': 1, 'perhap': 1, 'would': 1, 'find': 1, 'one': 2, 'good': 1, 'risk': 1}\n",
      " \n",
      "Some interesting stemmed words and their count:\n",
      "mechan - 2\n",
      "nois - 5\n",
      "turn - 2\n"
     ]
    }
   ],
   "source": [
    "print(x_train.iloc[0])\n",
    "print(' ')\n",
    "print(BOW_train[0])\n",
    "print(' ')\n",
    "print('Some interesting stemmed words and their count:')\n",
    "print('mechan -', BOW_train[0]['mechan'])\n",
    "print('nois -', BOW_train[0]['nois'])\n",
    "print('turn -', BOW_train[0]['turn'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f705f7",
   "metadata": {},
   "source": [
    "#### As you can see, after clean-up, we are only left with the stems of our essential words (i.e., words that WILL affect our sentiments). These stems are then turned into a dictionary as part of the 'bag-of-word' technique. I have kept the training set as it is for the sake of purity.\n",
    "\n",
    "#### In the above review from training set, there are two words with the stem 'mechan' viz., mechanics and mechanic. Both of these have been identified as we can see below. Similary the words 'noise' and 'noises' are identified by their stem word - 'nois', and the words 'turned' and 'turns' have been identified by their stem 'turn'. \n",
    "\n",
    "#### Also, notice that the word 'Six' with an upper case 'S' has been stemmed to 'six' with a lower case 's'. Same can be seen for other words like 'Ford' and 'Windstar'. \n",
    "\n",
    "##### Note: Use Ctrl + F to find all the instances of the above words/stems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cd371",
   "metadata": {},
   "source": [
    "### Step 3 - Creating vectors for our Algorithm\n",
    "\n",
    "#### For the purposes of this Lab, I have decided to use vectors of 1s and 0s despite counting the number of elements in the previous step. The counts, nonetheless, helped in visualizing the efficiency of my code. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693a68ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creates a 'Megalist' - list of all the unique stemwords from our training set\n",
    "megalist = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    for j in BOW_train[i]:\n",
    "        if j not in megalist:\n",
    "            megalist.append(j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754ec8f7",
   "metadata": {},
   "source": [
    "#### In the above cell, we created a 'megalist' of all the unique stemwords from our training set. Then in the cell below, we used the megalist as a backdrop to create our vectors of 1s and 0s, where '1' represents whether a particular word from the megalist is present in a given review and '0' represents the absence thereof. \n",
    "\n",
    "#### We use the same process on test set to get vectors as well. However, there are some words in test set that don't appear in the training set. Thus, we put an 'if' condition inside the for loops of test case accounting for this issue. It makes sure that the the code only accounts for the words that are IN the megalist and puts '0' for the one's that it doesn't recognize. \n",
    "\n",
    "##### The reason for creating a megalist in the first place was to avoid dimension errors in our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dc0fd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Starting with all zeros\n",
    "bayes_train_array = np.zeros((x_train.shape[0], len(megalist)))\n",
    "for i in range(x_train.shape[0]):\n",
    "    for j in BOW_train[i]:\n",
    "        bayes_train_array[i][megalist.index(j)] = 1\n",
    "\n",
    "#prints the first 5 rows for display\n",
    "print(bayes_train_array[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791c206c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Starting with all zeros\n",
    "bayes_test_array = np.zeros((x_test.shape[0], len(megalist)))\n",
    "for i in range(x_test.shape[0]):\n",
    "    for j in BOW_test[i]:\n",
    "        #This accounts for missing words in the megalist\n",
    "        if j in megalist:\n",
    "            bayes_test_array[i][megalist.index(j)] = 1\n",
    "\n",
    "#prints the first 5 rows for display\n",
    "print(bayes_test_array[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc4b481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10109\n",
      "10109\n",
      "10109\n"
     ]
    }
   ],
   "source": [
    "print(len(bayes_train_array[0]))\n",
    "print(len(bayes_test_array[0]))\n",
    "print(len(megalist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd887e9",
   "metadata": {},
   "source": [
    "### Step 4 - Using the Naive Bayes Classifier!!\n",
    "\n",
    "##### I opted to use the Multinomial NB tool from the scikit-learn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb20676b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit (train) out model according to our training set\n",
    "clf = MultinomialNB()\n",
    "clf.fit(bayes_train_array, y_train)\n",
    "#Then we predict sentiments for our test set\n",
    "R = clf.predict(bayes_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b5ddd5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix in terms of the number of reviews is:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg          106.0           28.0\n",
      "Actual Pos           24.0          118.0\n",
      " \n",
      "The confusion matrix in terms of the percentages is:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg      38.405797      10.144928\n",
      "Actual Pos       8.695652      42.753623\n"
     ]
    }
   ],
   "source": [
    "#A confusion matrix to test the accuracy of our algorithm\n",
    "conf_mat_NB = confusion_matrix(R, y_test)\n",
    "conf_mat_NB = conf_mat_NB/276\n",
    "\n",
    "#Creating a confusion matrix represented by actual number of reviews\n",
    "values_NB = {'Actual Neg': conf_mat_NB[0]*276, 'Actual Pos': conf_mat_NB[1]*276}\n",
    "finale_NB = pd.DataFrame.from_dict(values_NB, orient='index', columns=['Predicted Neg', 'Predicted Pos'])\n",
    "\n",
    "#Creating a confusion matrix represented by percentage of total reviews\n",
    "percentage_NB = {'Actual Neg': conf_mat_NB[0]*100.0, 'Actual Pos': conf_mat_NB[1]*100.0}\n",
    "felina_NB = pd.DataFrame.from_dict(percentage_NB, orient='index', columns=['Predicted Neg', 'Predicted Pos'])\n",
    "\n",
    "print('The confusion matrix in terms of the number of reviews is:')\n",
    "print(finale_NB)\n",
    "print(\" \")\n",
    "print('The confusion matrix in terms of the percentages is:')\n",
    "print(felina_NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6d93ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of Sentiments predicted correctly by the Naive Bayes method is:  81.15942028985508 %\n"
     ]
    }
   ],
   "source": [
    "#The percentage of correct predictions is the sum of true positive and true negatives\n",
    "Correct_predictions_NB = conf_mat_NB[0][0] + conf_mat_NB[1][1]\n",
    "print('The percentage of Sentiments predicted correctly by the Naive Bayes method is: ', Correct_predictions_NB*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd1885a",
   "metadata": {},
   "source": [
    "# Task 2 - Logistic Regression\n",
    "\n",
    "#### For my Task 2, I used an alternate classification algorithm viz., the Logistic Regression Algorithm. Incidentally, the scikit-learn library had tools to use logistic regression as well, and the inputs for this tools were identical to the ones required for Naive Bayes Algorithm. \n",
    "\n",
    "#### As for my reason behind using the Logistic Regression method, I was concerned about the independence assumption of the Naive Bayes classifier. The Naive Bayes Classifier assumes that all the variables in our dataset are independent of each other. However, when working with reviews, were words are part of sentences, this might not be completely true. It is possible that 'phrases' rather than 'words' are deterministic variables in predicting sentiments. If that were true, then individual words would NOT be independent of each other. In that case, Naive Bayes would be a far worse algorithm than Logistic regression which takes correlation in consideration. \n",
    "\n",
    "#### For reference, here is one of the articles that I found online which highlighted this point - https://dataespresso.com/en/2017/10/24/comparison-between-naive-bayes-and-logistic-regression/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26208ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We fit (train) out model according to our training set\n",
    "model = LogisticRegression()\n",
    "model.fit(bayes_train_array, y_train)\n",
    "#Then we predict sentiments for our test set\n",
    "Q = model.predict(bayes_test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "223b64de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The confusion matrix in terms of the number of reviews is:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg          104.0           29.0\n",
      "Actual Pos           26.0          117.0\n",
      " \n",
      "The confusion matrix in terms of the percentages is:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg      37.681159      10.507246\n",
      "Actual Pos       9.420290      42.391304\n"
     ]
    }
   ],
   "source": [
    "#A confusion matrix to test the accuracy of our algorithm\n",
    "conf_mat_LR = confusion_matrix(Q, y_test)\n",
    "conf_mat_LR = conf_mat_LR/276\n",
    "\n",
    "#Creating a confusion matrix represented by actual number of reviews\n",
    "values_LR = {'Actual Neg': conf_mat_LR[0]*276, 'Actual Pos': conf_mat_LR[1]*276}\n",
    "finale_LR = pd.DataFrame.from_dict(values_LR, orient='index', columns=['Predicted Neg', 'Predicted Pos'])\n",
    "\n",
    "#Creating a confusion matrix represented by percentage of total reviews\n",
    "percentage_LR = {'Actual Neg': conf_mat_LR[0]*100.0, 'Actual Pos': conf_mat_LR[1]*100.0}\n",
    "felina_LR = pd.DataFrame.from_dict(percentage_LR, orient='index', columns=['Predicted Neg', 'Predicted Pos'])\n",
    "\n",
    "print('The confusion matrix in terms of the number of reviews is:')\n",
    "print(finale_LR)\n",
    "print(\" \")\n",
    "print('The confusion matrix in terms of the percentages is:')\n",
    "print(felina_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4b764008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of Sentiments predicted correctly by the Logistic Regression method is:  80.07246376811594 %\n"
     ]
    }
   ],
   "source": [
    "#The percentage of correct predictions is the sum of true positive and true negatives\n",
    "Correct_predictions_LR = conf_mat_LR[0][0] + conf_mat_LR[1][1]\n",
    "print('The percentage of Sentiments predicted correctly by the Logistic Regression method is: ', Correct_predictions_LR*100,'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb87b23",
   "metadata": {},
   "source": [
    "### Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b4d5123d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percent-wise confusion matrix we got for Naive Bayes was:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg      38.405797      10.144928\n",
      "Actual Pos       8.695652      42.753623\n",
      " \n",
      "The percent-wise confusion matrix we got for Logistic Regression was:\n",
      "            Predicted Neg  Predicted Pos\n",
      "Actual Neg      37.681159      10.507246\n",
      "Actual Pos       9.420290      42.391304\n"
     ]
    }
   ],
   "source": [
    "print('The percent-wise confusion matrix we got for Naive Bayes was:')\n",
    "print(felina_NB)\n",
    "print(\" \")\n",
    "print('The percent-wise confusion matrix we got for Logistic Regression was:')\n",
    "print(felina_LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bf5fbf",
   "metadata": {},
   "source": [
    "#### On comparing the values of the above two confusion matrices, it is apparent that Logistic Regression did slightly worse than Naive Bayes. There doesn't seem to be any apparent pattern between the two matrices. The True positives and negatives are fewer and false negatives and false positives are higher. There seems to be a general failure in estimating parameters. The Naive Bayes parameters (posteriors) were perhaps a bit better than the Logistic Regression ones.  \n",
    "\n",
    "#### I think the reason that Logisitic Regression did worse than Naive Bayes in this case was because the Dataset was a bit small. Logisitic Regression does not perform well on smaller datasets. However, it is hard to know the threshold of these 'small' datasets. I assumed it would be big enough for it to work, but clearly it wasn't. From the looks of it, the parameters calculated by Logistic Regression were not good enough to give us a better value than the parameters calculated by Naive Bayes Classifier, since they were not trained enough. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eb548e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
